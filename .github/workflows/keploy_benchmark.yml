name: Keploy Benchmark Pipeline

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  TIME_FORMAT_STRING: "%e,%P,%M"
  REPORT_SHEET_NAME: "benchmark_report"
  NOTIFICATION_RECIPIENT: "asish.kumar.programmer@gmail.com"

jobs:
  build-keploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Keploy Source Code
        uses: actions/checkout@v4
      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.22"
      - name: Build Keploy Binary
        run: go build -o keploy
      - name: Upload Keploy Binary as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: build
          path: keploy
          retention-days: 1

  python-benchmark:
    needs: [build-keploy]
    runs-on: ubuntu-latest
    name: Python Django PostgreSQL Benchmark
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - id: record
        uses: ./.github/actions/download-binary
        with:
          src: build

      - id: replay
        uses: ./.github/actions/download-binary
        with:
          src: build

      - name: Download Keploy Binary Artifact
        uses: actions/download-artifact@v4
        with:
          name: build
          path: build

      - name: Set up Keploy Binary
        run: |
          chmod +x build/keploy
          echo "RECORD_BIN=${{ steps.record.outputs.path }}" >> $GITHUB_ENV
          echo "REPLAY_BIN=${{ steps.replay.outputs.path }}" >> $GITHUB_ENV
          echo "Keploy binary location: ${{ steps.record.outputs.path }}"
          ${{ steps.record.outputs.path }} --version || true

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Checkout the samples-python repository
        uses: actions/checkout@v4
        with:
          repository: keploy/samples-python
          path: samples-python

      - name: Install system dependencies
        run: |
          sudo apt-get update && sudo apt-get install -y bc
          echo "Current Docker Compose version:"
          docker compose version || docker-compose version || true

      - name: Run Python Django Benchmark
        id: python-benchmark
        env:
          RECORD_BIN: ${{ steps.record.outputs.path }}
          REPLAY_BIN: ${{ steps.replay.outputs.path }}
        run: |
          cd samples-python/django-postgres/django_postgres
          chmod +x ../../../.github/workflows/benchmark/python-benchmark.sh

          # Execute benchmark with metrics collection
          exec > >(tee benchmark_output.log) 2>&1

          echo "=== Starting Python Django PostgreSQL Benchmark ==="
          echo "Using RECORD_BIN: $RECORD_BIN"
          echo "Using REPLAY_BIN: $REPLAY_BIN"

          # Initialize metrics variables
          FINAL_STATUS="Success"
          FAILURE_REASON="None"
          REC_DURATION=0; REC_CPU_P="0%"; REC_MEM_MB=0
          TEST_DURATION=0; TEST_CPU_P="0%"; TEST_MEM_MB=0

          # Run the benchmark script and capture metrics
          if source ../../../.github/workflows/benchmark/python-benchmark.sh; then
            echo "=== Benchmark completed successfully ==="
            
            # Parse metrics from benchmark files
            if [ -f "record_metrics_1.txt" ]; then
              REC_LINE=$(cat record_metrics_1.txt | grep "Elapsed")
              REC_DURATION=$(echo "$REC_LINE" | grep -o 'Elapsed: [0-9.]*' | cut -d' ' -f2)
              REC_CPU_P=$(echo "$REC_LINE" | grep -o 'CPU: [0-9.]*%' | cut -d' ' -f2)
              REC_MEM_KB=$(echo "$REC_LINE" | grep -o 'Memory: [0-9]* KB' | cut -d' ' -f2)
              REC_MEM_MB=$(echo "scale=2; $REC_MEM_KB / 1024" | bc -l)
            fi
            
            if [ -f "test_metrics_detailed.txt" ]; then
              TEST_LINE=$(cat test_metrics_detailed.txt | grep "Elapsed")
              TEST_DURATION=$(echo "$TEST_LINE" | grep -o 'Elapsed: [0-9.]*' | cut -d' ' -f2)
              TEST_CPU_P=$(echo "$TEST_LINE" | grep -o 'CPU: [0-9.]*%' | cut -d' ' -f2)
              TEST_MEM_KB=$(echo "$TEST_LINE" | grep -o 'Memory Peak: [0-9]* KB' | cut -d' ' -f3)
              TEST_MEM_MB=$(echo "scale=2; $TEST_MEM_KB / 1024" | bc -l)
            fi
          else
            echo "=== Benchmark failed ==="
            FINAL_STATUS="Failed"
            FAILURE_REASON="Benchmark script execution failed"
          fi

          # Output metrics for other jobs
          echo "final_status=$FINAL_STATUS" >> $GITHUB_OUTPUT
          echo "failure_reason=$FAILURE_REASON" >> $GITHUB_OUTPUT
          echo "rec_duration=$REC_DURATION" >> $GITHUB_OUTPUT
          echo "rec_cpu_p=$REC_CPU_P" >> $GITHUB_OUTPUT
          echo "rec_mem_mb=$REC_MEM_MB" >> $GITHUB_OUTPUT
          echo "test_duration=$TEST_DURATION" >> $GITHUB_OUTPUT
          echo "test_cpu_p=$TEST_CPU_P" >> $GITHUB_OUTPUT
          echo "test_mem_mb=$TEST_MEM_MB" >> $GITHUB_OUTPUT

          if [ "$FINAL_STATUS" != "Success" ]; then exit 1; fi

      - name: Upload Benchmark Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: python-benchmark-results
          path: |
            samples-python/django-postgres/django_postgres/benchmark_output.log
            samples-python/django-postgres/django_postgres/record_metrics_*.txt
            samples-python/django-postgres/django_postgres/test_metrics_detailed.txt
            samples-python/django-postgres/django_postgres/keploy/reports/
          retention-days: 7

      - name: Display Benchmark Summary
        if: always()
        run: |
          cd samples-python/django-postgres/django_postgres
          echo "=== BENCHMARK SUMMARY ==="
          echo "Application: Python Django + PostgreSQL"
          echo "Timestamp: $(date)"
          echo "Workflow Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          echo ""
          echo "=== METRICS SUMMARY ==="

          # Display record metrics
          for i in {1..2}; do
            if [ -f "record_metrics_${i}.txt" ]; then
              echo "Record Cycle ${i}:"
              cat "record_metrics_${i}.txt"
            fi
          done

                    # Display test metrics
          if [ -f "test_metrics_detailed.txt" ]; then
            echo "Test Execution Metrics:"
            cat "test_metrics_detailed.txt"
          fi

      - name: Create Report Data for Google Sheets
        if: always()
        run: |
          cd samples-python/django-postgres/django_postgres
          TIMESTAMP_IST=$(TZ='Asia/Kolkata' date +'%b %d, %Y, %I:%M %p %Z')
          WORKFLOW_LINK="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          FINAL_STATUS="${{ steps.python-benchmark.outputs.final_status }}"
          FAILURE_REASON="${{ steps.python-benchmark.outputs.failure_reason }}"
          REC_DUR=${{ steps.python-benchmark.outputs.rec_duration || 0 }}
          REC_CPU_P="${{ steps.python-benchmark.outputs.rec_cpu_p || '0%' }}"
          REC_MEM_MB=${{ steps.python-benchmark.outputs.rec_mem_mb || 0 }}
          TEST_DUR=${{ steps.python-benchmark.outputs.test_duration || 0 }}
          TEST_CPU_P="${{ steps.python-benchmark.outputs.test_cpu_p || '0%' }}"
          TEST_MEM_MB=${{ steps.python-benchmark.outputs.test_mem_mb || 0 }}
          APPLICATION="Python Django + PostgreSQL"

          JSON_ROW=$(jq -n \
            --arg ts "$TIMESTAMP_IST" \
            --arg wlink "$WORKFLOW_LINK" \
            --arg app "$APPLICATION" \
            --arg status "$FINAL_STATUS" \
            --arg reason "$FAILURE_REASON" \
            --argjson recdur $REC_DUR \
            --arg reccpup "$REC_CPU_P" \
            --argjson recmem $REC_MEM_MB \
            --argjson testdur $TEST_DUR \
            --arg testcpup "$TEST_CPU_P" \
            --argjson testmem $TEST_MEM_MB \
            '[[$ts, $wlink, $app, $status, $reason, $recdur, $reccpup, $recmem, $testdur, $testcpup, $testmem]]')
          echo "$JSON_ROW" > ../../../python_report.json

      - name: Upload Report Data as Artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: python-benchmark-report
          path: python_report.json

      - name: Prepare Attachments on Failure
        if: failure()
        run: |
          echo "Zipping Keploy directory for email attachment..."
          cd samples-python/django-postgres/django_postgres
          if [ -d "keploy" ]; then 
            sudo zip -r ../../../keploy_artifacts.zip keploy
          else 
            echo "Keploy directory not found."
          fi

      - name: Send Email Notification on Failure
        if: failure()
        uses: dawidd6/action-send-mail@v3
        with:
          server_address: "smtp.gmail.com"
          server_port: 587
          username: ${{ secrets.GMAIL_USERNAME }}
          password: ${{ secrets.GMAIL_APP_PASSWORD }}
          subject: "‚ùå CI Failure: Keploy Benchmark Failed for Python Django"
          to: ${{ env.NOTIFICATION_RECIPIENT }}
          from: GitHub Actions CI <${{ secrets.GMAIL_USERNAME }}>
          body: |
            A failure occurred while running the Keploy benchmark pipeline.
            **Application:** Python Django + PostgreSQL
            **Failure Reason:** `${{ steps.python-benchmark.outputs.failure_reason }}`
            **Logs:** See benchmark_output.log artifact.
            **Run:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          attachments: samples-python/django-postgres/django_postgres/benchmark_output.log, keploy_artifacts.zip

  # Future: Add more language benchmarks
  # golang-benchmark:
  #   needs: [build-keploy]
  #   runs-on: ubuntu-latest
  #   # Similar structure for Go benchmarks

  # nodejs-benchmark:
  #   needs: [build-keploy]
  #   runs-on: ubuntu-latest
  #   # Similar structure for Node.js benchmarks

  report-to-google-sheets:
    runs-on: ubuntu-latest
    needs: [python-benchmark]
    if: always()
    steps:
      - name: Authenticate to Google Cloud
        id: auth
        uses: "google-github-actions/auth@v2"
        with:
          create_credentials_file: false
          credentials_json: "${{ secrets.GCP_SA_KEY }}"
          access_token_scopes: "https://www.googleapis.com/auth/spreadsheets"

      - name: Download all benchmark reports
        uses: actions/download-artifact@v4
        with:
          path: all-reports
          pattern: "*-benchmark-report"
          merge-multiple: true

      - name: Aggregate all reports into a single payload
        id: aggregate
        run: |
          files=$(find all-reports -name "*_report.json" -type f)
          if [ -z "$files" ]; then 
            echo "payload=[]" >> $GITHUB_OUTPUT
          else 
            FINAL_PAYLOAD=$(jq -s 'map(.[0])' $files)
            echo "payload=${FINAL_PAYLOAD}" >> $GITHUB_OUTPUT
          fi

      - name: Append data to Google Sheet via REST API
        if: steps.aggregate.outputs.payload != '[]'
        run: |
          API_URL="https://sheets.googleapis.com/v4/spreadsheets/${{ secrets.SHEET_ID }}/values/${{ env.REPORT_SHEET_NAME }}:append?valueInputOption=USER_ENTERED"
          JSON_BODY=$(jq -n --argjson values "${{ steps.aggregate.outputs.payload }}" '{ "values": $values }')
          curl -s --request POST "$API_URL" \
               --header "Authorization: Bearer ${{ steps.auth.outputs.access_token }}" \
               --header "Content-Type: application/json" \
               --data-raw "$JSON_BODY"

  benchmark-summary:
    runs-on: ubuntu-latest
    needs: [python-benchmark]
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: all-benchmark-results
          pattern: "*-benchmark-results"
          merge-multiple: true

      - name: Generate Benchmark Report
        run: |
          echo "# Keploy Benchmark Report" > benchmark_report.md
          echo "" >> benchmark_report.md
          echo "**Generated:** $(date)" >> benchmark_report.md
          echo "**Workflow:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> benchmark_report.md
          echo "" >> benchmark_report.md

          echo "## Summary" >> benchmark_report.md
          echo "- **Python Django PostgreSQL**: ${{ needs.python-benchmark.result }}" >> benchmark_report.md
          echo "" >> benchmark_report.md

          echo "## Detailed Results" >> benchmark_report.md
          if [ -f all-benchmark-results/benchmark_output.log ]; then
            echo "### Python Django PostgreSQL Benchmark" >> benchmark_report.md
            echo '```' >> benchmark_report.md
            cat all-benchmark-results/benchmark_output.log >> benchmark_report.md
            echo '```' >> benchmark_report.md
          fi

      - name: Upload Final Benchmark Report
        uses: actions/upload-artifact@v4
        with:
          name: keploy-benchmark-report
          path: benchmark_report.md
          retention-days: 30
