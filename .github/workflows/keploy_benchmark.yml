name: Keploy Benchmark Pipeline

on:
  push:
    branches: [main]
  workflow_dispatch:

env:
  TIME_FORMAT_STRING: "%e,%P,%M"

jobs:
  build-keploy:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout Keploy Source Code
        uses: actions/checkout@v4
      - name: Set up Go
        uses: actions/setup-go@v5
        with:
          go-version: "1.22"
      - name: Build Keploy Binary
        run: go build -o keploy
      - name: Upload Keploy Binary as Artifact
        uses: actions/upload-artifact@v4
        with:
          name: build
          path: keploy
          retention-days: 1

  python-benchmark:
    needs: [build-keploy]
    runs-on: ubuntu-latest
    name: Python Django PostgreSQL Benchmark
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - id: record
        uses: ./.github/actions/download-binary
        with:
          src: build

      - id: replay
        uses: ./.github/actions/download-binary
        with:
          src: build

      - name: Download Keploy Binary Artifact
        uses: actions/download-artifact@v4
        with:
          name: build
          path: build

      - name: Set up Keploy Binary
        run: |
          chmod +x build/keploy
          echo "RECORD_BIN=${{ steps.record.outputs.path }}" >> $GITHUB_ENV
          echo "REPLAY_BIN=${{ steps.replay.outputs.path }}" >> $GITHUB_ENV
          echo "Keploy binary location: ${{ steps.record.outputs.path }}"
          ${{ steps.record.outputs.path }} --version || true

      - name: Checkout the samples-python repository
        uses: actions/checkout@v4
        with:
          repository: keploy/samples-python
          path: samples-python

      - name: Install system dependencies
        run: |
          sudo apt-get update && sudo apt-get install -y bc
          echo "Current Docker Compose version:"
          docker compose version || docker-compose version || true

      - name: Run Python Django Benchmark
        id: python-benchmark
        env:
          RECORD_BIN: ${{ steps.record.outputs.path }}
          REPLAY_BIN: ${{ steps.replay.outputs.path }}
        run: |
          cd samples-python/django-postgres/django_postgres
          chmod +x ../../../.github/workflows/benchmark/python-benchmark.sh

          # Execute benchmark with metrics collection
          exec > >(tee benchmark_output.log) 2>&1

          echo "=== Starting Python Django PostgreSQL Benchmark ==="
          echo "Using RECORD_BIN: $RECORD_BIN"
          echo "Using REPLAY_BIN: $REPLAY_BIN"

          # Run the benchmark script
          source ../../../.github/workflows/benchmark/python-benchmark.sh

          echo "=== Benchmark completed successfully ==="

      - name: Upload Benchmark Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: python-benchmark-results
          path: |
            samples-python/django-postgres/django_postgres/benchmark_output.log
            samples-python/django-postgres/django_postgres/record_metrics_*.txt
            samples-python/django-postgres/django_postgres/test_metrics_detailed.txt
            samples-python/django-postgres/django_postgres/keploy/reports/
          retention-days: 7

      - name: Display Benchmark Summary
        if: always()
        run: |
          cd samples-python/django-postgres/django_postgres
          echo "=== BENCHMARK SUMMARY ==="
          echo "Application: Python Django + PostgreSQL"
          echo "Timestamp: $(date)"
          echo "Workflow Run: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"

          echo ""
          echo "=== METRICS SUMMARY ==="

          # Display record metrics
          for i in {1..2}; do
            if [ -f "record_metrics_${i}.txt" ]; then
              echo "Record Cycle ${i}:"
              cat "record_metrics_${i}.txt"
            fi
          done

          # Display test metrics
          if [ -f "test_metrics_detailed.txt" ]; then
            echo "Test Execution Metrics:"
            cat "test_metrics_detailed.txt"
          fi

  # Future: Add more language benchmarks
  # golang-benchmark:
  #   needs: [build-keploy]
  #   runs-on: ubuntu-latest
  #   # Similar structure for Go benchmarks

  # nodejs-benchmark:
  #   needs: [build-keploy]
  #   runs-on: ubuntu-latest
  #   # Similar structure for Node.js benchmarks

  benchmark-summary:
    runs-on: ubuntu-latest
    needs: [python-benchmark]
    if: always()
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all benchmark results
        uses: actions/download-artifact@v4
        with:
          path: all-benchmark-results
          pattern: "*-benchmark-results"
          merge-multiple: true

      - name: Generate Benchmark Report
        run: |
          echo "# Keploy Benchmark Report" > benchmark_report.md
          echo "" >> benchmark_report.md
          echo "**Generated:** $(date)" >> benchmark_report.md
          echo "**Workflow:** ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}" >> benchmark_report.md
          echo "" >> benchmark_report.md

          echo "## Summary" >> benchmark_report.md
          echo "- **Python Django PostgreSQL**: ${{ needs.python-benchmark.result }}" >> benchmark_report.md
          echo "" >> benchmark_report.md

          echo "## Detailed Results" >> benchmark_report.md
          if [ -f all-benchmark-results/benchmark_output.log ]; then
            echo "### Python Django PostgreSQL Benchmark" >> benchmark_report.md
            echo '```' >> benchmark_report.md
            cat all-benchmark-results/benchmark_output.log >> benchmark_report.md
            echo '```' >> benchmark_report.md
          fi

      - name: Upload Final Benchmark Report
        uses: actions/upload-artifact@v4
        with:
          name: keploy-benchmark-report
          path: benchmark_report.md
          retention-days: 30
