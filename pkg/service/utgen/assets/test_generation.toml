[test_generation]
system="""\
"""

user="""\
## Overview
You are a code assistant designed to accept a {{ .language }} source file and a {{ .language }} test file. 
Your task is to generate additional unit tests to complement the existing test suite, aiming to significantly increase the code coverage of the source file.

### Requirements for Creating Tests:

- **Analyze the Provided Code:**
  - Understand its purpose, inputs, outputs, and key logic or calculations.
  - **Identify Return Types:**
    - Determine the data types of return values for each function or method.
    - Use return type information to guide the creation of relevant test cases.

- **Refactor for Testability:**
  - **Refactor the provided source code to improve testability**, including making external dependencies easily mockable, especially for asynchronous interactions.
  - Ensure refactoring enhances testability without altering functionality or breaking existing behavior.
  - Provide refactored code in the `refactored_source_code` field if changes are made.
  - **Refactoring Techniques:**
    - Use dependency injection to manage dependencies.
    - Separate concerns to isolate different parts of the code.
    - Implement interfaces or abstract classes to make components easily mockable.

- **Utilize the Code Coverage Report:**
  - Identify specific parts of the code not yet covered by tests.
  - Focus on uncovered lines, branches, and conditions.
  - **Highlight Critical Areas:**
    - Prioritize testing for high-risk or critical sections of the code.
  - **Coverage Metrics:**
    - Aim for a minimum coverage threshold (e.g., 80%) and provide guidance on interpreting coverage metrics.

- **Generate Targeted Test Cases:**
  - Write tests for uncovered code paths, including within functions that already have tests.
  - Include edge cases, error conditions, and scenarios with complex or async logic.
  - **Boundary Conditions:**
    - Test boundary values and limits.
  - **Concurrency and Performance:**
    - Include tests that assess concurrency or performance where applicable.
  - **Security and Validation:**
    - Write tests that validate input sanitization, authentication, and authorization where applicable.
  - **Data Type Specific Tests:**
    - **Validate Return Types:**
      - Ensure that functions return data of the expected type.
      - Create tests that check the integrity and structure of the returned data.
    - **Type-Based Scenarios:**
      - Generate test cases based on different data types (e.g., strings, integers, objects, arrays) to cover various input and output scenarios.

- **Use Mocks and Stubs:**
  - Where appropriate, simulate complex dependencies or external interactions.
  - For asynchronous operations, use async-compatible mocking methods.
  - Test for async edge cases, ensuring proper event loop handling and responses.
  - **Mocking Strategies:**
    - Use appropriate libraries (e.g., `unittest.mock` for Python, Mockito for Java).
    - Simulate external API calls with predefined responses.
    - Mock asynchronous functions using libraries compatible with async operations.
    - Dont Mock Databases/Redis/Any Client

- **Maximize Coverage:**
  - Try to include as many functions and code paths as possible.
  - Cover all branches, error handling paths, and edge cases.
  - **Comprehensive Data Coverage:**
    - Ensure that all possible data types and structures returned by functions are adequately tested.
    - Include tests for both typical and atypical data types where applicable.

- **Ensure Quality and Consistency:**
  - Write comprehensive, well-structured tests.
  - Follow the style and conventions of the existing test suite.
  - Ensure test names are unique within the test suite.
  - **Best Practices:**
    - Adhere to naming conventions (e.g., `test_methodName_condition_expectedResult`).
    - Add docstrings or comments within tests to explain their purpose.
    - Avoid redundant tests by cross-referencing test behaviors.
    - **Data Type Validation:**
      - Incorporate checks to verify that returned data types match expected types.

- **Focus on the Goal:**
  - The primary objective is to **increase the overall code coverage significantly**.
  - Do not include the code coverage report or any policies in your response.

{{- if .function_under_test }}
- **Focus Function:**  
  - You must generate test cases specifically targeting the function named `{{ .function_under_test }}`.  
  - Ensure that the tests for this function cover all logic paths, edge cases, and error handling scenarios.
  - **Data Type Consideration:**
    - Analyze the return type of `{{ .function_under_test }}` and create tests that validate the correctness and integrity of the returned data.
{{ end }}

{{- if .additional_command }}
- {{ .additional_command }}
{{ end }}


## Source File
Here is the source file that you will be writing tests against, called `{{ .source_file_name }}`. Line numbers have been added for clarity and are not part of the original code.
=========
{{ .source_file_numbered | trim }}
=========


## Test File
Here is the file that contains the existing tests, called `{{ .test_file_name }}`.
=========
{{ .test_file | trim }}
=========

## Installed Packages
The following packages are already installed in the environment. Use these when writing tests to avoid redundant installations:

=========
{{ .installed_packages | trim }}
=========


{%- if additional_includes_section | trim %}
{{ .additional_includes_section | trim }}
{% endif %}

{%- if failed_tests_section | trim  %}

{{ .failed_tests_section | trim }}
{% endif %}

{%- if additional_instructions_text | trim  %}

{{ .additional_instructions_text | trim }}
{% endif %}


## Code Coverage
The following is the existing code coverage report. Use this to determine what tests to write, as you should only write tests that increase the overall coverage:
=========
{{ .code_coverage_report| trim }}
=========

## Refactoring Guidelines
To improve testability without altering functionality, consider the following refactoring techniques:
- **Dependency Injection:** Pass dependencies as parameters to functions or constructors.
- **Separation of Concerns:** Isolate different parts of the code to simplify testing.
- **Use of Interfaces/Abstract Classes:** Define interfaces or abstract classes for components to facilitate mocking.

Provide any refactored source code in the `refactored_source_code` field if changes are made.

## Mocking Strategies
When simulating dependencies or external interactions:
- Use appropriate mocking libraries based on the language (e.g., `unittest.mock` for Python, Mockito for Java).
- Simulate external API calls with predefined responses.
- Mock asynchronous functions using libraries compatible with async operations.

Ensure that mocks accurately represent the behavior of the actual dependencies to maintain test reliability.

## Best Practices and Standards
- **Naming Conventions:** Follow a consistent naming pattern for tests, such as `test_methodName_condition_expectedResult`.
- **Test Documentation:** Include docstrings or comments to explain the purpose and logic of each test case.
- **Avoid Redundancy:** Ensure new tests are not duplicating existing ones by cross-referencing test behaviors.
- **Data Type Validation:** Incorporate checks to verify that returned data types match expected types.

## Feedback Mechanism
- **Review and Iterate:** Periodically review generated tests to identify gaps or areas for improvement.
- **User Feedback Integration:** Allow users to provide feedback on the usefulness and coverage of generated tests to refine the generation logic.

## Handling Complex Scenarios
Address more intricate testing scenarios to ensure comprehensive coverage:
- **Integration Tests:** Consider how integration tests fit into the overall testing strategy alongside unit tests.
- **Stateful Components:** Provide guidance on testing components that maintain state or have side effects.

## YAML Response Structure
Ensure the YAML output adheres to the expected schema and is optimized for readability and maintainability:
- **Consistent Formatting:** Maintain uniform indentation and structure.
- **Modular Sections:** Organize the YAML into manageable sections.
- **Validation:** Ensure the YAML is free from syntax errors and conforms to the required schema.

## Response
The output must be a YAML object equivalent to type $NewTests, according to the following Pydantic definitions:
=====
class SingleTest(BaseModel):
    test_behavior: str = Field(description="Short description of the behavior the test covers")
{%- if language in ["python","java"] %}
    test_name: str = Field(description=" A short test name, in snake case, that reflects the behaviour to test")
{%- else %}
    test_name: str = Field(description=" A short unique test name, that should reflect the test objective")
{%- endif %}
    test_code: str = Field(description="A single test function, that tests the behavior described in 'test_behavior'. The test should be a written like its a part of the existing test suite, if there is one, and it can use existing helper functions, setup, or teardown code.")
    new_imports_code: str = Field(description="Code for new imports that are required for the new test function, and are not already present in the test file.")
    library_installation_code: str = Field(description="If new libraries are needed, specify the installation commands for each library separately.")
    test_tags: str = Field(description="A single label that best describes the test, out of: ['happy path', 'edge case','other']")

class NewTests(BaseModel):
    language: str = Field(description="The programming language of the source code")
    existing_test_function_signature: str = Field(description="A single line repeating a signature header of one of the existing test functions")
    new_tests: List[SingleTest] = Field(min_items=1, max_items={{ .max_tests }}, description="A list of new test functions to append to the existing test suite, aiming to increase the code coverage. Each test should run as-is, without requiring any additional inputs or setup code.")
    refactored_source_code: str = Field(description="The refactored source code that improves testability while retaining original functionality.")

=====
    
Example output:
```yaml
language: {{ .language }}
existing_test_function_signature: |
  ...
new_tests:
- test_behavior: |
    Test that the function returns the correct output for a single element list
{%- if language in ["python","java"] %}
  test_name: |
    test_single_element_list
{%- else %}
  test_name: |
    ...
{%- endif %}
  test_code: |
{%- if language in ["python"] %}
    import pytest
    @pytest.mark.asyncio
    def ...
{%- else %}
    ...
{%- endif %}
  new_imports_code: |
    {%- if language == "python" %}
    import pytest
    {%- elif language == "java" %}
    import org.junit.jupiter.api.Test;
    import my.package.MyFunction;
    {%- elif language == "golang" %}
    import "testing"
    import "my_module"
    {%- elif language == "javascript" %}
    const assert = require('assert');
    const myFunction = require('my_module').myFunction;
    {%- elif language == "typescript" %}
    import { assert } from 'assert';
    import { myFunction } from 'my_module';
    {%- endif %}
  library_installation_code: |
    {%- if language == "python" %}
    pip install pytest
    {%- elif language == "java" %}
    # Add the following to your Maven pom.xml:
    # <dependency>
    #   <groupId>org.junit.jupiter</groupId>
    #   <artifactId>junit-jupiter-api</artifactId>
    #   <version>5.7.0</version>
    #   <scope>test</scope>
    # </dependency>
    {%- elif language == "golang" %}
    go get github.com/my_module
    {%- elif language == "javascript" %}
    npm install assert
    {%- elif language == "typescript" %}
    npm install assert
    {%- endif %}
  test_tags: happy path

refactored_source_code: |
  # Here is the modified source code that retains original functionality but improves testability.
  ...
```

Use block scalar('|') to format each YAML output.


{%- if additional_instructions_text| trim  %}

{{ .additional_instructions_text| trim }}
{% endif %}


In Python, since this can be an issue Unclosed client session client_session: <aiohttp.client.ClientSession object at 0x102fed130>, please ensure using pytest-aiohttp to solve this issue
In Python, Since  async def functions are not natively supported for testing and can be skipped, please add the decorator @pytest.mark.asyncio to the test functions that are async def functions, also if you are using the decorator, please import pytest and pytest-asyncio as well.

Response (should be a valid YAML, and nothing else):
```yaml
"""
