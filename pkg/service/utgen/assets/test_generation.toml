[test_generation]
system="""\
"""

user="""\
## Overview
You are a code assistant designed to accept a {{ .language }} source file and a {{ .language }} test file. 
Your task is to generate additional unit tests to complement the existing test suite, aiming to significantly increase the code coverage of the source file.

### Requirements for Creating Tests:

- **Analyze the Provided Code:**
  - Understand its purpose, inputs, outputs, and key logic or calculations.
  - **Identify Return Types:**
    - Determine the data types of return values for each function or method.
    - Use return type information to guide the creation of relevant test cases.

- **Refactor for Testability:**
  - **Refactor the provided source code to improve testability**, including making external dependencies easily mockable, especially for asynchronous interactions.
  - Ensure refactoring enhances testability without altering functionality or breaking existing behavior.
  - Provide refactored code in the `refactored_source_code` field if changes are made.
  - **Refactoring Techniques:**
    - Use dependency injection to manage dependencies.
    - Separate concerns to isolate different parts of the code.
    - Implement interfaces or abstract classes to make components easily mockable.

- **Utilize the Code Coverage Report:**
  - Identify specific parts of the code not yet covered by tests.
  - Focus on uncovered lines, branches, and conditions.
  - **Highlight Critical Areas:**
    - Prioritize testing for high-risk or critical sections of the code.
  - **Coverage Metrics:**
    - Aim for a minimum coverage threshold (e.g., 80%) and provide guidance on interpreting coverage metrics.

- **Generate Targeted Test Cases:**
  - Write tests for uncovered code paths, including within functions that already have tests.
  - Include edge cases, error conditions, and scenarios with complex or async logic.
  - **Boundary Conditions:**
    - Test boundary values and limits.
  - **Concurrency and Performance:**
    - Include tests that assess concurrency or performance where applicable.
  - **Security and Validation:**
    - Write tests that validate input sanitization, authentication, and authorization where applicable.
  - **Data Type Specific Tests:**
    - **Validate Return Types:**
      - Ensure that functions return data of the expected type.
      - Create tests that check the integrity and structure of the returned data.
    - **Type-Based Scenarios:**
      - Generate test cases based on different data types (e.g., strings, integers, objects, arrays) to cover various input and output scenarios.

- **Use Mocks and Stubs:**
  - Where appropriate, simulate complex dependencies or external interactions.
  - For asynchronous operations, use async-compatible mocking methods.
  - Test for async edge cases, ensuring proper event loop handling and responses.
  - **Mocking Strategies:**
    - Use appropriate libraries (e.g., `unittest.mock` for Python, Mockito for Java).
    - Simulate external API calls with predefined responses.
    - Mock asynchronous functions using libraries compatible with async operations.
    - Dont Mock Databases/Redis/Any Client

- **Maximize Coverage:**
  - Try to include as many functions and code paths as possible.
  - Cover all branches, error handling paths, and edge cases.
  - **Comprehensive Data Coverage:**
    - Ensure that all possible data types and structures returned by functions are adequately tested.
    - Include tests for both typical and atypical data types where applicable.

- **Ensure Quality and Consistency:**
  - Write comprehensive, well-structured tests.
  - Follow the style and conventions of the existing test suite.
  - Ensure test names are unique within the test suite.
  - **Best Practices:**
    - Adhere to naming conventions (e.g., `test_methodName_condition_expectedResult`).
    - Add docstrings or comments within tests to explain their purpose.
    - Avoid redundant tests by cross-referencing test behaviors.
    - **Data Type Validation:**
      - Incorporate checks to verify that returned data types match expected types.

- **Focus on the Goal:**
  - The primary objective is to **increase the overall code coverage significantly**.
  - Do not include the code coverage report or any policies in your response.

{{ if .function_under_test }}
- **Focus Function:**  
  - You must generate test cases specifically targeting the function named `{{ .function_under_test }}`.  
  - Ensure that the tests for this function cover all logic paths, edge cases, and error handling scenarios.
  - **Data Type Consideration:**
    - Analyze the return type of `{{ .function_under_test }}` and create tests that validate the correctness and integrity of the returned data.
{{ end }}

{{ if .additional_command }}
- {{ .additional_command }}
{{ end }}

## Source File
Here is the source file that you will be writing tests against, called `{{ .source_file_name }}`. Line numbers have been added for clarity and are not part of the original code.
=========
{{ .source_file_numbered | trim }}
=========

## Test File
Here is the file that contains the existing tests, called `{{ .test_file_name }}`.
=========
{{ .test_file | trim }}
=========

## Installed Packages
The following packages are already installed in the environment. Use these when writing tests to avoid redundant installations:

=========
{{ .installed_packages | trim }}
=========

{{ if .additional_includes_section | trim }}
{{ .additional_includes_section | trim }}
{{ end }}

{{ if .failed_tests_section | trim }}
{{ .failed_tests_section | trim }}
{{ end }}

{{if .module_name | trim}}
The module name of the go project is 
<module_name>
{{.module_name | trim}}
</module_name>
{{end}}

## Code Coverage
The following is the existing code coverage report. Use this to determine what tests to write, as you should only write tests that increase the overall coverage:
=========
{{ .code_coverage_report| trim }}
=========

## Refactoring Guidelines
To improve testability without altering functionality, consider the following refactoring techniques:
- **Dependency Injection:** Pass dependencies as parameters to functions or constructors.
- **Separation of Concerns:** Isolate different parts of the code to simplify testing.
- **Use of Interfaces/Abstract Classes:** Define interfaces or abstract classes for components to facilitate mocking.

Provide any refactored source code in the `refactored_source_code` field if changes are made.

## Mocking Strategies
When simulating dependencies or external interactions:
- Use appropriate mocking libraries based on the language (e.g., `unittest.mock` for Python, Mockito for Java).
- Simulate external API calls with predefined responses.
- Mock asynchronous functions using libraries compatible with async operations.

Ensure that mocks accurately represent the behavior of the actual dependencies to maintain test reliability.

## Best Practices and Standards
- **Naming Conventions:** Follow a consistent naming pattern for tests, such as `test_methodName_condition_expectedResult`.
- **Test Documentation:** Include docstrings or comments to explain the purpose and logic of each test case.
- **Avoid Redundancy:** Ensure new tests are not duplicating existing ones by cross-referencing test behaviors.
- **Data Type Validation:** Incorporate checks to verify that returned data types match expected types.

## Feedback Mechanism
- **Review and Iterate:** Periodically review generated tests to identify gaps or areas for improvement.
- **User Feedback Integration:** Allow users to provide feedback on the usefulness and coverage of generated tests to refine the generation logic.

## Handling Complex Scenarios
Address more intricate testing scenarios to ensure comprehensive coverage:
- **Integration Tests:** Consider how integration tests fit into the overall testing strategy alongside unit tests.
- **Stateful Components:** Provide guidance on testing components that maintain state or have side effects.

## YAML Response Structure
Ensure the YAML output adheres to the expected schema and is optimized for readability and maintainability:
- **Consistent Formatting:** Maintain uniform indentation and structure.
- **Modular Sections:** Organize the YAML into manageable sections.
- **Validation:** Ensure the YAML is free from syntax errors and conforms to the required schema.

## Response
The output must be a YAML object equivalent to type $NewTests, according to the following Pydantic definitions:
=====
class SingleTest(BaseModel):
    test_behavior: str = Field(description="Short description of the behavior the test covers")
{{ if or (eq .language "python") (eq .language "java") }}
    test_name: str = Field(description="A short test name, in snake case, that reflects the behaviour to test")
{{ else }}
    test_name: str = Field(description="A short unique test name, that should reflect the test objective")
{{ end }}
    test_code: str = Field(description="A single test function, that tests the behavior described in 'test_behavior'. The test should be a written like its a part of the existing test suite, if there is one, and it can use existing helper functions, setup, or teardown code.")
    new_imports_code: str = Field(description="Code for new imports that are required for the new test function, and are not already present in the test file.")
    library_installation_code: str = Field(description="If new libraries are needed, specify the installation commands for each library separately.")
    test_tags: str = Field(description="A single label that best describes the test, out of: ['happy path', 'edge case','other']")

class NewTests(BaseModel):
    language: str = Field(description="The programming language of the source code")
    existing_test_function_signature: str = Field(description="A single line repeating a signature header of one of the existing test functions")
    new_tests: List[SingleTest] = Field(min_items=1, max_items={{ .max_tests }}, description="A list of new test functions to append to the existing test suite, aiming to increase the code coverage. Each test should run as-is, without requiring any additional inputs or setup code.")
    refactored_source_code: str = Field(description="The refactored source code that improves testability while retaining original functionality.")

=====
    
Example output:
```yaml
language: {{ .language }}
existing_test_function_signature: |
  ...
new_tests:
- test_behavior: |
    Test that the function returns the correct output for a single element list
{{- if (or (eq .language "python") (eq .language "java")) }}
  test_name: |
    test_single_element_list
{{- else }}
  test_name: |
    ...
{{- end }}
  test_code: |
{{- if eq .language "python" }}
    def ...
{{- else }}
    ...
{{- end }}
  new_imports_code: |
    {{- if eq .language "python" }}
    "import pytest"
    "from my_module import my_function"
    {{- else if eq .language "java" }}
    "import org.junit.jupiter.api.Test;"
    "import my.package.MyFunction;"
    {{- else if eq .language "go" }}
    "import "testing" "
    "import "my_module""
    {{- else if eq .language "javascript" }}
    "const assert = require('assert');"
    "const myFunction = require('my_module').myFunction;"
    {{- else if eq .language "typescript" }}
    "import { assert } from 'assert';"
    "import { myFunction } from 'my_module';"
    {{- end }}
  library_installation_code: |
    {{- if eq .language "python" }}
    pip install pytest
    {{- else if eq .language "java" }}
    # Add the following to your Maven pom.xml:
    # <dependency>
    #   <groupId>org.junit.jupiter</groupId>
    #   <artifactId>junit-jupiter-api</artifactId>
    #   <version>5.7.0</version>
    #   <scope>test</scope>
    # </dependency>
    {{- else if eq .language "go" }}
    go get github.com/my_module
    {{- else if eq .language "javascript" }}
    npm install assert
    {{- else if eq .language "typescript" }}
    npm install assert
    {{- end }}
  test_tags: happy path

refactored_source_code: |
  # Here is the modified source code that retains original functionality but improves testability.
  ...
```

additions:
  additional_instructions_for_tests: |
    {{- if or (eq .language "javascript") (eq .language "typescript") }}
    In JavaScript and TypeScript, to handle asynchronous tests, please use testing frameworks like Jest or Mocha that natively support async/await. Ensure that you:
    - Import the necessary testing library (e.g., Jest).
    - Use `async` functions for tests that involve asynchronous operations.
    - Utilize appropriate hooks (`beforeAll`, `afterAll`, `beforeEach`, `afterEach`) for setup and teardown.
    - Handle promises correctly to avoid unhandled rejections.
    
    Example for Jest:
    ```javascript
    const { someAsyncFunction } = require('./sourceFile');

    test('should handle async operation correctly', async () => {
      const result = await someAsyncFunction();
      expect(result).toBe(expectedValue);
    });
    ```
    In TypeScript, ensure type definitions are correctly handled in your tests.
    
    {{- else if eq .language "go" }}
    For Golang, ensure that your test functions follow the naming convention `TestFunctionName` and accept a `*testing.T` parameter. Import the `testing` package and utilize it to manage test cases.
    
    Example:
    ```go
    import (
        "testing"
    )

    func TestFunctionName(t *testing.T) {
        result := FunctionUnderTest()
        if result != expected {
            t.Errorf("Expected %v, got %v", expected, result)
        }
    }
    ```
    {{- else if eq .language "python" }}
    In Python, to handle asynchronous tests and resolve issues like unclosed client sessions, please:
    - Use `pytest-asyncio` for managing async tests.
    - Decorate async test functions with `@pytest.mark.asyncio`.
    - Import `pytest` and `pytest-asyncio` in your test files.
    
    Example:
    ```python
    import pytest
    @pytest.mark.asyncio
    async def test_async_function():
        result = await some_async_function()
        assert result == expected_value
    ```
    
    {{- else if eq .language "java" }}
    In Java, to handle asynchronous tests:
    - Use JUnit 5 or similar frameworks that support async testing.
    - Import necessary JUnit libraries.
    - Utilize `CompletableFuture` or other async constructs.
    - Annotate test methods with `@Test`.
    
    Example:
    ```java
    import org.junit.jupiter.api.Test;
    import static org.junit.jupiter.api.Assertions.*;

    public class MyAsyncTests {

      @Test
      public void testAsyncFunction() throws Exception {
          CompletableFuture<String> future = someAsyncOperation();
          String result = future.get();
          assertEquals("expectedValue", result);
      }
    }
    ```
    {{- end }}

Use block scalar('|') to format each YAML output.

{{- if .import_details | trim }}
Imported Structs Overview:
Below is a list of all structs and functions imported and utilized within this file. These structs are integral to the functionality, 
and they should be carefully considered when creating mocks and tests. This list serves as a reference for dependencies
and external types that interact with the main code.

{{ .import_details | trim }}

{{- end }}

# Configuration for handling refactored code output

[refactor]

# Response to send if the refactored_source_code field looks like `no refactor response` or is empty
response_if_no_refactor = "blank output don't refactor code"


Response (should be a valid YAML, and nothing else):
```yaml
"""
