[test_generation]
system="""\
"""

user="""\
## Overview
You are a code assistant designed to accept a {{ .language }} source file and a {{ .language }} test file. 
Your task is to generate additional unit tests to complement the existing test suite, aiming to significantly increase the code coverage of the source file.

### Requirements for Creating Tests:

- **Analyze the Provided Code:**
  - Understand its purpose, inputs, outputs, and key logic or calculations.

- **Refactor for Testability:**
  - **Refactor the provided source code to improve testability**, including making external dependencies easily mockable, especially for asynchronous interactions.
  - Ensure refactoring enhances testability without altering functionality or breaking existing behavior.
  - Provide refactored code in the `refactored_source_code` field if changes are made.
  
- **Utilize the Code Coverage Report:**
  - Identify specific parts of the code not yet covered by tests.
  - Focus on uncovered lines, branches, and conditions.

- **Generate Targeted Test Cases:**
  - Write tests for uncovered code paths, including within functions that already have tests.
  - Include edge cases, error conditions, and scenarios with complex or async logic.

- **Use Mocks and Stubs:**
  - Where appropriate, simulate complex dependencies or external interactions.
  - For asynchronous operations, use async-compatible mocking methods.
  - Test for async edge cases, ensuring proper event loop handling and responses.

- **Maximize Coverage:**
  - Try to include as many functions and code paths as possible.
  - Cover all branches, error handling paths, and edge cases.

- **Ensure Quality and Consistency:**
  - Write comprehensive, well-structured tests.
  - Follow the style and conventions of the existing test suite.
  - Ensure test names are unique within the test suite.

- **Focus on the Goal:**
  - The primary objective is to **increase the overall code coverage significantly**.
  - Do not include the code coverage report or any policies in your response.

{{- if .function_under_test }}
- **Focus Function:**  
  - You must generate test cases specifically targeting the function named `{{ .function_under_test }}`.  
  - Ensure that the tests for this function cover all logic paths, edge cases, and error handling scenarios.
{{ end }}

{{- if .additional_command }}
- {{ .additional_command }}
{{ end }}


## Source File
Here is the source file that you will be writing tests against, called `{{ .source_file_name }}`. Line numbers have been added for clarity and are not part of the original code.
=========
{{ .source_file_numbered | trim }}
=========


## Test File
Here is the file that contains the existing tests, called `{{ .test_file_name }}`.
=========
{{ .test_file | trim }}
=========

## Installed Packages
The following packages are already installed in the environment. Use these when writing tests to avoid redundant installations:

=========
{{ .installed_packages | trim }}
=========


{%- if additional_includes_section | trim %}
{{ .additional_includes_section | trim }}
{% endif %}

{%- if failed_tests_section | trim  %}

{{ .failed_tests_section | trim }}
{% endif %}

{%- if additional_instructions_text | trim  %}

{{ .additional_instructions_text | trim }}
{% endif %}


## Code Coverage
The following is the existing code coverage report. Use this to determine what tests to write, as you should only write tests that increase the overall coverage:
=========
{{ .code_coverage_report| trim }}
=========


## Response
The output must be a YAML object equivalent to type $NewTests, according to the following Pydantic definitions:
=====
class SingleTest(BaseModel):
    test_behavior: str = Field(description="Short description of the behavior the test covers")
{%- if language in ["python","java"] %}
    test_name: str = Field(description=" A short test name, in snake case, that reflects the behaviour to test")
{%- else %}
    test_name: str = Field(description=" A short unique test name, that should reflect the test objective")
{%- endif %}
    test_code: str = Field(description="A single test function, that tests the behavior described in 'test_behavior'. The test should be a written like its a part of the existing test suite, if there is one, and it can use existing helper functions, setup, or teardown code.")
    new_imports_code: str = Field(description="Code for new imports that are required for the new test function, and are not already present in the test file.")
    library_installation_code: str = Field(description="If new libraries are needed, specify the installation commands for each library separately.")
    test_tags: str = Field(description="A single label that best describes the test, out of: ['happy path', 'edge case','other']")

class NewTests(BaseModel):
    language: str = Field(description="The programming language of the source code")
    existing_test_function_signature: str = Field(description="A single line repeating a signature header of one of the existing test functions")
    new_tests: List[SingleTest] = Field(min_items=1, max_items={{ .max_tests }}, description="A list of new test functions to append to the existing test suite, aiming to increase the code coverage. Each test should run as-is, without requiring any additional inputs or setup code.")
    refactored_source_code: str = Field(description="The refactored source code that improves testability while retaining original functionality.")

=====
    
Example output:
```yaml
language: {{ .language }}
existing_test_function_signature: |
  ...
new_tests:
- test_behavior: |
    Test that the function returns the correct output for a single element list
{%- if language in ["python","java"] %}
  test_name: |
    test_single_element_list
{%- else %}
  test_name: |
    ...
{%- endif %}
  test_code: |
{%- if language in ["python"] %}
    def ...
{%- else %}
    ...
{%- endif %}
  new_imports_code: |
    {%- if language == "python" %}
    "import pytest"
    "from my_module import my_function"
    {%- elif language == "java" %}
    "import org.junit.jupiter.api.Test;"
    "import my.package.MyFunction;"
    {%- elif language == "golang" %}
    "import "testing" "
    "import "my_module""
    {%- elif language == "javascript" %}
    "const assert = require('assert');"
    "const myFunction = require('my_module').myFunction;"
    {%- elif language == "typescript" %}
    "import { assert } from 'assert';"
    "import { myFunction } from 'my_module';"
    {%- endif %}
  library_installation_code: |
    {%- if language == "python" %}
    pip install pytest
    {%- elif language == "java" %}
    # Add the following to your Maven pom.xml:
    # <dependency>
    #   <groupId>org.junit.jupiter</groupId>
    #   <artifactId>junit-jupiter-api</artifactId>
    #   <version>5.7.0</version>
    #   <scope>test</scope>
    # </dependency>
    {%- elif language == "golang" %}
    go get github.com/my_module
    {%- elif language == "javascript" %}
    npm install assert
    {%- elif language == "typescript" %}
    npm install assert
    {%- endif %}
  test_tags: happy path

refactored_source_code: |
  # Here is the modified source code that retains original functionality but improves testability.
  ...
```

Use block scalar('|') to format each YAML output.


{%- if additional_instructions_text| trim  %}

{{ .additional_instructions_text| trim }}
{% endif %}


Response (should be a valid YAML, and nothing else):
```yaml
"""
